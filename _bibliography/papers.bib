---
---

@misc{https://doi.org/10.48550/arxiv.2203.08445,
  abbr = {EMNLP},
  author = {Dua, Dheeru and Gupta, Shivanshu and Gardner, Matt and Singh, Sameer},
  title = {Successive Prompting for Decomposing Complex Questions},
  year = {2022},
  abstract = "Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass.  We introduce ``Successive Prompting'', where we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate a synthetic dataset which can be used to bootstrap a model's ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement of âˆ¼5% absolute F1 on a few-shot version of the DROP dataset when compared with a state-of-the-art model with the same supervision",
  code = "https://github.com/dDua/succesive_prompting",
  selected={false},
}

@misc{https://doi.org/10.48550/arxiv.2203.08445,
  abbr = {EMNLP Findings},
  doi = {10.48550/ARXIV.2203.08445},
  url = {https://arxiv.org/abs/2203.08445},
  author = {Gupta, Shivanshu and Singh, Sameer and Gardner, Matt},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license},
  abstract = "A rapidly growing body of research has demonstrated the inability of NLP models to generalize compositionally and has tried to alleviate it through specialized architectures, training schemes, and data augmentation, among other approaches. In this work, we study a different relatively under-explored approach: sampling diverse train sets that encourage compositional generalization. We propose a novel algorithm for sampling a structurally diverse set of instances from a labeled instance pool with structured outputs. Evaluating on 5 semantic parsing datasets of varying complexity, we show that our algorithm performs competitively with or better than prior algorithms in not only compositional template splits but also traditional IID splits of all but the least structurally diverse datasets. In general, we find that diverse train sets lead to better generalization than random training sets of the same size in 9 out of 10 dataset-split pairs, with over 10% absolute improvement in 5, providing further evidence to their sample efficiency. Moreover, we show that structural diversity also makes for more comprehensive test sets that require diverse training to succeed on. Finally, we use information theory to show that reduction in spurious correlations between substructures may be one reason why diverse training sets improve generalization.",

  code = "https://github.com/Shivanshu-Gupta/structural-diversity",
  arxiv="2203.08445",
  pdf="https://arxiv.org/pdf/2203.08445.pdf",
  bibtex_show={true},
  selected={true},
}


@misc{https://doi.org/10.48550/arxiv.2201.05899,
  abbr = {EMNLP},
  doi = {10.48550/ARXIV.2201.05899},
  url = {https://arxiv.org/abs/2201.05899},
  author = {Bogin, Ben and Gupta, Shivanshu and Berant, Jonathan},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Unobserved Local Structures Make Compositional Generalization Hard},
  abstract = {While recent work has convincingly showed that sequence-to-sequence models struggle to generalize to new compositions (termed compositional generalization), little is known on what makes compositional generalization hard on a particular test instance. In this work, we investigate what are the factors that make generalization to certain test instances challenging. We first substantiate that indeed some examples are more difficult than others by showing that different models consistently fail or succeed on the same test instances. Then, we propose a criterion for the difficulty of an example: a test instance is hard if it contains a local structure that was not observed at training time. We formulate a simple decision rule based on this criterion and empirically show it predicts instance-level generalization well across 5 different semantic parsing datasets, substantially better than alternative decision rules. Last, we show local structures can be leveraged for creating difficult adversarial compositional splits and also to improve compositional generalization under limited training budgets by strategically selecting examples for the training set.},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license},

  code = "https://github.com/benbogin/unobserved-local-structures",
  arxiv="2201.05899",
  pdf="https://arxiv.org/pdf/2201.05899.pdf",
  selected={true},
  bibtex_show={true},
}


@inproceedings{bogin-etal-2021-covr,
  abbr={EMNLP},
  title = "{COVR}: A Test-Bed for Visually Grounded Compositional Generalization with Real Images",
  author = "Bogin, Ben  and
    Gupta, Shivanshu  and
    Gardner, Matt  and
    Berant, Jonathan",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2021",
  address = "Online and Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.emnlp-main.774",
  doi = "10.18653/v1/2021.emnlp-main.774",
  pages = "9824--9846",
  abstract = "While interest in models that generalize at test time to new compositions has risen in recent years, benchmarks in the visually-grounded domain have thus far been restricted to synthetic images. In this work, we propose COVR, a new test-bed for visually-grounded compositional generalization with real images. To create COVR, we use real images annotated with scene graphs, and propose an almost fully automatic procedure for generating question-answer pairs along with a set of context images. COVR focuses on questions that require complex reasoning, including higher-order operations such as quantification and aggregation. Due to the automatic generation process, COVR facilitates the creation of compositional splits, where models at test time need to generalize to new concepts and compositions in a zero- or few-shot setting. We construct compositional splits using COVR and demonstrate a myriad of cases where state-of-the-art pre-trained language-and-vision models struggle to compositionally generalize.",

  code = "https://github.com/benbogin/covr-dataset",
  arxiv="2109.10613",
  pdf="https://aclanthology.org/2021.emnlp-main.774.pdf",
  website = "https://covr-dataset.github.io/",
  selected={true},
  bibtex_show={true},
}
